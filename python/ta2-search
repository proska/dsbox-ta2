#!/usr/bin/env python

"""
Command Line Interface for running the DSBox TA2 Search
"""
import time
from dsbox_dev_setup import path_setup
import argparse
import json
import os
import signal
import subprocess
import sys
import traceback
from pprint import pprint
from sklearn import metrics, preprocessing

import pandas as pd
from importlib import reload
import dsbox.controller.controller

reload(dsbox.controller.controller)
from dsbox.controller.controller import Controller
from dsbox.controller.controller import Status
import os
import psutil
# controller = Controller(development_mode=True)

import getpass

start_time = time.time()

def main(args):
    timeout = 0
    configuration_file = args.configuration_file
    debug = args.debug

    controller = Controller(development_mode=debug)

    with open(configuration_file) as data:
        config = json.load(data)

    # Time to write results (in minutes)
    write_results_time = 2
    if args.timeout:
        timeout = args.timeout - write_results_time
    else:
        if 'timeout' in config:
            # Timeout less 1 minute to give system chance to clean up
            timeout = int(config['timeout']) - write_results_time
        else:
            timeout = 60 - write_results_time
            config['timeout'] = timeout

    print('[INFO] Time out is ', timeout)

    def kill_child_processes():
        print("[INFO]Killing child processes")
        process_id = os.getpid()
        parent = psutil.Process(process_id)
        for child in parent.children(recursive=True):  # or parent.children() for recursive=False
            child.kill()

    # Define signal handler to exit gracefully
    def write_results_and_exit(a_signal, frame):
        print('==== Times up ====')
        time_used = (time.time() - start_time) / 60.0
        print("[INFO] The time used so far is {:0.2f} minutes.".format(time_used))
        try:
            # Reset to handlers to default as not to output multiple times
            signal.signal(signal.SIGALRM, signal.SIG_DFL)

            print('[INFO] Killing child processes', flush=True)

            print('[INFO] writing results', flush=True)
            controller.write_training_results()

            print('==== Done cleaning up ====', flush=True)
            time_used = (time.time() - start_time) / 60.0
            print("[INFO] The time used so far is {:0.2f} minutes.".format(time_used), flush=True)

            kill_child_processes()
        except Exception as e:
            print(e)
            traceback.print_exc()
        finally:
            # sys.exit(0) generates SystemExit exception, which may
            # be caught and ignored.

            # This os._exit() cannot be caught.
            # print('SIGNAL exiting {}'.format(configuration_file), flush=True)
            best_pipeline_id = None
            if args.test:
                best_pipeline_id = run_single_test()

            if args.test_generated_pipelines:
                run_generated_pipelines(best_pipeline_id)

            if args.score:
                try:
                    scoring()
                except:
                    traceback.print_exc()
                    pass

            kill_child_processes()
            os._exit(0)

    def run_single_test():
        print("[INFO] Pick the best pipeline and run a test")
        test_config_single_pipeline = json.load(open(args.test, 'r'))
        test_controller_single_pipeline = Controller(development_mode=False)
        test_controller_single_pipeline.initialize_from_config_for_evaluation(test_config_single_pipeline)

        rank_lst = list()
        for pipeline in [os.path.join(controller.output_pipelines_dir, x) for x in
                         os.listdir(controller.output_pipelines_dir) if x.endswith(".json")]:
            try:
                pipeline_json = json.load(open(pipeline, 'r'))
                rank_lst.append((pipeline_json["pipeline_rank"], pipeline_json['id']))
            except:
                pass
        best_fitted_pipeline_id = min(rank_lst)[1]
        test_controller_single_pipeline.test_fitted_pipeline(fitted_pipeline_id=best_fitted_pipeline_id)
        return best_fitted_pipeline_id

    def run_generated_pipelines(best_pipeline_id=None):
        print("[INFO] Pick all pipelines and run test")
        test_config = json.load(open(args.test_generated_pipelines, 'r'))
        test_controller = Controller(development_mode=False)
        test_controller.initialize_from_config_for_evaluation(test_config)

        for pipeline in [os.path.join(controller.output_pipelines_dir, x) for x in
                         os.listdir(controller.output_pipelines_dir) if x.endswith(".json")]:
            try:
                pipeline_json = json.load(open(pipeline, 'r'))
                fitted_pipeline_id = pipeline_json['id']
                if fitted_pipeline_id != best_pipeline_id:
                    test_controller.test_fitted_pipeline(fitted_pipeline_id=fitted_pipeline_id)
            except:
                traceback.print_exc()
                pass

    def scoring():
        if args.test:
            prediction_path = json.load(open(args.test, 'r'))["results_root"]
        elif args.test_generated_pipelines:
            prediction_path = json.load(open(args.test_generated_pipelines, 'r'))["results_root"]
        else:
            return

        training_data_root = config["training_data_root"]
        solution_root = "/".join(
            config["training_data_root"].split("/")[:-1] + ["{}_solution".format(config["training_data_root"].split("/")[-2])])

        ground_truth_idx = pd.read_csv(os.path.join(solution_root, "predictions.csv"), index_col='d3mIndex').index
        ground_truth = pd.read_csv(os.path.join(training_data_root, "tables/learningData.csv"),
                                   index_col='d3mIndex').loc[ground_truth_idx, :]

        dataset_schema = json.load(open(config["dataset_schema"], "r"))
        for col in dataset_schema['dataResources'][0]['columns']:
            if col['colName'] != "d3mIndex" and 'suggestedTarget' in col['role']:
                target = col['colName']
                break
        ground_truth = ground_truth.loc[:, target]
        scores = dict()
        metric = None
        for prediction_dir in os.listdir(prediction_path):
            if os.path.isdir(os.path.join(prediction_path, prediction_dir)):
                try:
                    if not metric:
                        metric = \
                            json.load(
                                open(os.path.join(controller.output_pipelines_dir, "{}.json".format(prediction_dir))))[
                                "metric"]
                    prediction = pd.read_csv(os.path.join(prediction_path, prediction_dir, "predictions.csv"),
                                             index_col='d3mIndex').loc[ground_truth_idx, :]

                    if metric == "f1Macro":
                        score = metrics.f1_score(ground_truth, prediction, average='macro')
                    elif metric == "meanSquaredError":
                        score = metrics.mean_squared_error(ground_truth, prediction)
                    else:
                        score = None
                    scores[prediction_dir] = score
                except:
                    traceback.print_exc()
                    pass
        with open(os.path.join(prediction_path, 'scores.json'), 'w') as scores_f:
            json.dump(scores, scores_f, indent=2)

    if timeout > 0:
        signal.signal(signal.SIGALRM, write_results_and_exit)
        signal.alarm(60 * timeout)
    else:
        # Do not set alaram
        pass

    if args.cpus > -1:
        config['cpus'] = args.cpus

    # Replace output directories
    if args.output_prefix is not None:
        for key in ['pipeline_logs_root', 'executables_root', 'temp_storage_root']:
            if not '/output/' in config[key]:
                print(
                    'Skipping. No "*/output/" for config[{}]={}.'.format(key, config[key]))
            else:
                suffix = config[key].split('/output/', 1)[1]
                config[key] = os.path.join(args.output_prefix, suffix)

    # os.system('clear')
    print('Using configuation:')
    pprint(config)

    if 'test_data_schema' in config and config['test_data_schema'] != '':
        print("[INFO] Test data config found! Will use given test data.")
        controller.initialize_from_config(config)
    else:
        print("[INFO] No test data config found! Will split the data.")
        controller.initialize_from_config_train_test(config)

    status = controller.train()
    print("*+" * 10)
    controller.write_training_results()
    print("*+" * 10)
    #status = controller.test()
    #print("[INFO] Testing Done")

    time_used = (time.time() - start_time) / 60.0
    print("[INFO] The time used for running program is {:0.2f} minutes.".format(time_used))

    return status.value


class StdoutLogger(object):
    def __init__(self, f):
        self.terminal = sys.stdout
        self.log = f

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.log.flush()


class StderrLogger(object):
    def __init__(self, f):
        self.err = sys.stderr
        self.log = f

    def write(self, message):
        self.err.write(message)
        self.log.write(message)

    def flush(self):
        self.log.flush()


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description='Run DSBox TA2 system using json configuration file')

    parser.add_argument('configuration_file',
                        help='D3M TA2 json configuration file')
    parser.add_argument('--timeout', action='store', type=int, default=-1,
                        help='Overide configuation timeout setting. In minutes.')
    parser.add_argument('--cpus', action='store', type=int, default=-1,
                        help='Overide configuation number of cpus usage setting')
    parser.add_argument('--output-prefix', action='store', default=None,
                        help='''Overide configuation output directories paths (
                        pipeline_logs_root, executables_root, temp_storage_root).
                        Replace path prefix "*/output/" with argument''')
    parser.add_argument('--debug', action='store_true', default=False,
                        help='Debug mode. No timeout and no output redirection')

    parser.add_argument('--test', action='store', default=None,
                        help='Run a single test after search, need a test config')

    parser.add_argument('--test_generated_pipelines', action='store', default=None,
                        help='After search, run all generated pipelines on test data, need a test config')

    parser.add_argument('--score', action='store_true', default=False,
                        help='Calculate score for each prediction')

    args = parser.parse_args()

    config = json.load(open(args.configuration_file, "r"))
    if 'logs_root' in config:
        std_dir = os.path.abspath(config['logs_root'])
    else:
        std_dir = os.path.join(config['temp_storage_root'], 'logs')

    os.makedirs(std_dir, exist_ok=True)

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr
    f = open(os.path.join(std_dir, 'out.txt'), 'w')

    sys.stdout = StdoutLogger(f)
    sys.stderr = StderrLogger(f)

    print(args)

    result = main(args)
    sys.stdout = orig_stdout
    sys.stderr = orig_stderr

    f.close()

    os._exit(result)
